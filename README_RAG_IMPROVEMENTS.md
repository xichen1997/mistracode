# Codebase Agent RAG 改进功能

## 智能 RAG 决策功能

### 概述
改进后的 chat 功能现在能够智能判断用户查询是否需要使用 RAG（检索增强生成）搜索，还是直接回答。

### 主要改进

#### 1. 智能查询分类
- **代码相关问题**：自动使用 RAG 搜索相关代码片段
- **一般性问题**：直接回答，无需搜索代码库

#### 2. 关键词识别
**需要 RAG 的关键词：**
- 代码相关：代码、函数、类、方法、文件、实现、定义、调用、引用
- 问题相关：如何、怎么、什么、哪里、哪个、为什么、原因
- 搜索相关：搜索、查找、找到、定位、位置、行号、文件路径
- 分析相关：解释、说明、分析、理解、查看、检查、测试
- 项目相关：代码库、项目、模块、包、依赖、配置、设置

**不需要 RAG 的关键词：**
- 问候：你好、再见、谢谢、帮助、hello、hi
- 一般话题：天气、时间、日期、新闻、故事、笑话
- 通用概念：一般、通用、概念、理论、原理、基础知识

#### 3. 特殊模式
- **强制 RAG 模式**：使用 `/rag` 命令强制使用 RAG 搜索
- **强制直接回答模式**：使用 `/direct` 命令强制直接回答

### 使用方法

#### 1. 基本使用
```bash
# 启动交互式聊天
python codebase_agent_rag.py chat

# 测试 RAG 决策
python codebase_agent_rag.py test-rag-decision "这个函数是做什么的？"
```

#### 2. 交互式命令
在 chat 模式下，可以使用以下特殊命令：
- `/help` - 显示帮助信息
- `/stats` - 显示索引统计
- `/clear` - 清屏
- `/rag` - 强制使用 RAG 搜索下一个查询
- `/direct` - 强制直接回答下一个查询

#### 3. 测试功能
```bash
# 运行基本测试脚本
python test_rag_decision.py

# 运行 LLM 判断测试
python test_llm_judgment.py

# 交互式测试
python test_llm_judgment.py interactive

# 运行改进后的全面测试
python test_improved_rag.py

# 调试特定查询的 LLM 判断
python codebase_agent_rag.py debug-llm-judgment "如何实现这个功能？"
```

### 示例

#### 代码相关问题（使用 RAG）
```
用户: 这个函数是做什么的？
系统: 🔍 检测到代码相关问题，使用 RAG 搜索...
```

#### 一般性问题（直接回答）
```
用户: 你好
系统: 💬 检测到一般性问题，直接回答...
```

#### 强制模式
```
用户: /rag
系统: 已设置强制使用 RAG 搜索模式
用户: 今天天气怎么样？
系统: 🔍 强制使用 RAG 搜索...
```

#### LLM 智能判断
```
用户: 什么是 Python？
系统: 🤔 正在分析查询类型...
系统: 🤖 LLM 判断: DIRECT (直接回答)
系统: 💬 检测到一般性问题，直接回答...
```

```
用户: 这个函数是做什么的？
系统: 🤔 正在分析查询类型...
系统: 🤖 LLM 判断: RAG (需要搜索代码库)
系统: 🔍 检测到代码相关问题，使用 RAG 搜索...
```

### 优势

1. **提高效率**：避免对一般性问题进行不必要的代码搜索
2. **更快的响应**：直接回答比 RAG 搜索更快
3. **智能判断**：基于关键词和上下文智能分类查询
4. **用户控制**：提供强制模式让用户控制行为
5. **更好的用户体验**：清晰的反馈显示当前使用的模式

### 技术实现

#### 智能判断策略
1. **快速检查**：
   - 短查询（<3字符）直接返回 False
   - 明显问候语（你好、hi、hello 等）直接返回 False
   - 明显代码相关问题（包含"这个函数"、"代码中"、"错误"等）直接返回 True
   - 文件扩展名检测（.py, .js, .java 等）
   - 编程语法检测（括号、分号等）

2. **LLM 智能判断**：
   - 对于模糊的查询，使用 LLM 进行智能分析
   - 提供详细的判断标准和示例给 LLM
   - 特别处理边界情况（如"如何实现这个功能？"、"如何运行这个程序？"）
   - 要求 LLM 只回答 "RAG" 或 "DIRECT"
   - 失败时使用保守策略（默认不使用 RAG）
   - 显示 LLM 的完整响应用于调试

#### 优势
- **高效**：快速检查避免不必要的 LLM 调用
- **智能**：LLM 判断处理复杂和模糊的查询
- **可靠**：失败时使用保守策略确保系统稳定
- **透明**：显示判断过程和结果
- **可配置**：支持强制模式覆盖自动判断 