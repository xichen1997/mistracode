# Codebase Agent RAG Improvement Features

## Intelligent RAG Decision Functionality

### Overview
The improved chat functionality can now intelligently determine whether user queries need to use RAG (Retrieval-Augmented Generation) search or direct answers.

### Main Improvements

#### 1. Intelligent Query Classification
- **Code-related questions**: Automatically use RAG to search relevant code snippets
- **General questions**: Direct answers without needing to search the codebase

#### 2. Keyword Recognition
**Keywords that require RAG:**
- Code-related: code, function, class, method, file, implementation, definition, call, reference
- Question-related: how, what, where, which, why, reason
- Search-related: search, find, locate, position, line number, file path
- Analysis-related: explain, describe, analyze, understand, view, check, test
- Project-related: codebase, project, module, package, dependency, configuration, settings

**Keywords that don't require RAG:**
- Greetings: hello, goodbye, thank you, help, hi
- General topics: weather, time, date, news, story, joke
- General concepts: general, universal, concept, theory, principle, basic knowledge

#### 3. Special Modes
- **Forced RAG Mode**: Use `/rag` command to force RAG search
- **Forced Direct Answer Mode**: Use `/direct` command to force direct answers

### Usage

#### 1. Basic Usage
```bash
# Start interactive chat
python codebase_agent_rag.py chat

# Test RAG decision
python codebase_agent_rag.py test-rag-decision "What does this function do?"
```

#### 2. Interactive Commands
In chat mode, you can use the following special commands:
- `/help` - Show help information
- `/stats` - Show index statistics
- `/clear` - Clear screen
- `/rag` - Force RAG search for the next query
- `/direct` - Force direct answer for the next query

#### 3. Testing Features
```bash
# Run basic test script
python test_rag_decision.py

# Run LLM judgment test
python test_llm_judgment.py

# Interactive testing
python test_llm_judgment.py interactive

# Run comprehensive improved test
python test_improved_rag.py

# Debug LLM judgment for specific queries
python codebase_agent_rag.py debug-llm-judgment "How to implement this feature?"
```

### Examples

#### Code-related Questions (Using RAG)
```
User: What does this function do?
System: üîç Detected code-related question, using RAG search...
```

#### General Questions (Direct Answer)
```
User: Hello
System: üí¨ Detected general question, answering directly...
```

#### Forced Modes
```
User: /rag
System: Forced RAG search mode enabled
User: How's the weather today?
System: üîç Forcing RAG search...
```

#### LLM Intelligent Judgment
```
User: What is Python?
System: ü§î Analyzing query type...
System: ü§ñ LLM judgment: DIRECT (direct answer)
System: üí¨ Detected general question, answering directly...
```

```
User: What does this function do?
System: ü§î Analyzing query type...
System: ü§ñ LLM judgment: RAG (needs codebase search)
System: üîç Detected code-related question, using RAG search...
```

### Advantages

1. **Improved Efficiency**: Avoid unnecessary code searches for general questions
2. **Faster Response**: Direct answers are faster than RAG searches
3. **Intelligent Judgment**: Smart query classification based on keywords and context
4. **User Control**: Provide forced modes for user control
5. **Better User Experience**: Clear feedback showing current mode

### Technical Implementation

#### Intelligent Judgment Strategy
1. **Quick Check**:
   - Short queries (<3 characters) directly return False
   - Obvious greetings (hello, hi, etc.) directly return False
   - Obvious code-related questions (containing "this function", "in code", "error", etc.) directly return True
   - File extension detection (.py, .js, .java, etc.)
   - Programming syntax detection (parentheses, semicolons, etc.)

2. **LLM Intelligent Judgment**:
   - For ambiguous queries, use LLM for intelligent analysis
   - Provide detailed judgment criteria and examples to LLM
   - Special handling for edge cases (such as "How to implement this feature?", "How to run this program?")
   - Require LLM to only answer "RAG" or "DIRECT"
   - Use conservative strategy on failure (default to not using RAG)
   - Display complete LLM response for debugging

#### Advantages
- **Efficient**: Quick checks avoid unnecessary LLM calls
- **Intelligent**: LLM judgment handles complex and ambiguous queries
- **Reliable**: Conservative strategy ensures system stability on failure
- **Transparent**: Display judgment process and results
- **Configurable**: Support forced modes to override automatic judgment 